Amazon S3
=========

This page describes how to set up serving of media files through Amazon S3. It also outlines a few other useful S3 procedures.


S3 bucket setup
---------------

- Go to the Amazon S3 console.
- Create a bucket.
- Click your bucket's name, then click Properties, then Permissions.
- Click "Add CORS Configuration" and accept the default configuration.
- Click "Add bucket policy" and add the following policy, replacing ``<bucket-name>`` with your bucket's name and ``<mydomain.com>`` with your website's domain name:

::

  {
    "Version": "2012-10-17",
    "Statement": [
      {
        "Sid": "Allow referral from my domains",
        "Effect": "Allow",
        "Principal": "*",
        "Action": "s3:GetObject",
        "Resource": "arn:aws:s3:::<bucket-name>/*",
        "Condition": {
          "StringLike": {
            "aws:Referer": "http://<mydomain.com>/*"
          }
        }
      }
    ]
  }

Even if this bucket is just for a development copy of the website, the bucket generally has to be public to serve to the website. So, this bucket policy has a simple security measure that requires the Referer to be the website domain. This means that one cannot access the bucket resources simply by typing in URLs.

Production should `enable bucket versioning <https://docs.aws.amazon.com/AmazonS3/latest/UG/enable-bucket-versioning.html>`__.


Explanation on CORS and bucket policy
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The default CORS configuration should allow any website domain to include bucket objects in their webpages, provided that they have read access to those bucket objects in the first place. CORS is required because our actual website isn't on S3; therefore including an S3 resource on our website is cross-origin sharing.

The above bucket policy defines how the bucket objects can be accessed. It says that, if a bucket object is linked to from the given referer website, that object can always be read. In other cases, as long as our website code only writes S3 ACLs with private permissions, access is restricted to (1) the bucket-owning AWS account, and to (2) IAM users as defined in the IAM console.

This bucket policy prevents hotlinking to S3 images, meaning that if a user just types an image URL into their address bar, they will be denied access. For example, if a user sees an example patch from a private source with filename ``abcdefghij.jpg.150x150.jpg``, and then they try to reach the full-resolution image by simply cutting off the ``.150x150.jpg`` from the URL, they will be denied access.

Here's how it works: an S3 image is served with extra URL arguments after the filename, namely ``Signature=<letters and numbers>``, ``Expires=<number>``, and ``AWSAccessKeyId=<letters and numbers>``. The Signature in particular is generated by S3 during the website request, and is the key that allows the user to read the image. The Signature only works for serving that particular file until that particular Expires time (``django-storages`` makes it expire in 1 hour by default).

Note that although the AWS access key ID is exposed in the URL, it's not particularly a security issue, since an attacker cannot do anything unless they have the secret key as well. (`Link <http://stackoverflow.com/questions/7678835/how-secure-are-amazon-aws-access-keys>`__)

One potential security hole is the fact that ``HTTP_REFERER`` can be set by the web client. Most web clients provide valid referers, so it would take a reasonably focused snooper to take advantage of the security hole. Still, there are ways of restricting access further that could be implemented, such as using CloudFront and CNAMEs. See the comments in `this thread <http://stackoverflow.com/a/11525941/>`__.

See `this blog post <https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/>`__ for info on bucket policies, `this docs page <http://docs.aws.amazon.com/AmazonS3/latest/dev/manage-acls-using-console.html>`__ for info on ACLs, and `this docs page <http://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-4>`__ for the referer check.


.. _s3cmd:

Installing s3cmd
----------------

s3cmd is a third-party utility for Amazon S3 which provides some functionality that the AWS CLI lacks. This shouldn't be needed for normal website operation, but it can be useful if you need to reorganize S3 buckets for any reason. So, install this when needed.

Install with ``pip install s3cmd``.

- This doesn't have to be part of the CoralNet virtualenv, since we don't (yet) plan to run s3cmd from the Django project.
- Note that `s3cmd doesn't support Python 3 <https://github.com/s3tools/s3cmd/issues/335>`__ (as of 2016.11).

If you're on an EC2 instance, just make sure the instance is associated with an IAM Role which has S3 permissions.

If you're not on an EC2 instance, use ``s3cmd --configure`` to set your AWS access and secret keys. The configuration gets saved in ``~/.s3cfg``.


Syncing files from an S3 bucket to another S3 bucket
----------------------------------------------------

From an EC2 instance :ref:`with the AWS CLI installed <aws-cli>`, simply run: ``aws s3 sync s3://<source bucket> s3://<destination bucket>``

Note that S3 buckets cannot be renamed, so that boils down to a sync procedure as well: create a new bucket, sync files from the old bucket to the new one, and delete the old bucket.


Syncing files from a server's local filesystem to S3
----------------------------------------------------

The :ref:`AWS CLI <aws-cli>` can sync from a filesystem to S3 with the ``aws s3 sync`` command. However, it seems to hang without transferring anything in some cases with large directories. (`Related GitHub issue <https://github.com/aws/aws-cli/issues/1775>`__)

To avoid hanging, use s3cmd instead. The syntax is ``s3cmd sync LOCAL_DIR s3://BUCKET[/PREFIX]``.

- As the `s3cmd usage reference <http://s3tools.org/usage>`__ says, this "checks files freshness using size and md5 checksum, unless overridden by options". Add the option ``--no-check-md5`` to skip checking the md5 checksum, which should speed up the sync significantly. This should be a safe option for our image data, since the website doesn't have any way to edit previously uploaded image files.

- Presence of trailing slashes matters (`Link <http://s3tools.org/s3cmd-sync>`__). If source doesn't have a trailing slash, you'll end up with an extra directory at the end of your destination, like ``media/images/original/<filename>``. If destination doesn't have a trailing slash, s3cmd just forces an error. You need both trailing slashes.

- You may see "remote copy:" output lines which indicate an attempt to optimize away some network transfer: if two files are detected as having identical contents (from the size and md5 checksum), then instead of transferring both of those files from source to destination, s3cmd will copy file 1 from the source's file 1 and then copy file 2 from the destination's file 1. This doesn't seem to be explained explicitly anywhere, but the intended behavior can be guessed from s3cmd's verbose output and links like`this one <https://github.com/s3tools/s3cmd/issues/643>`__.

  - There's some potential for incorrect behavior though, so watch out: `Link 1 <https://github.com/s3tools/s3cmd/issues/768>`__, `Link 2 <http://stackoverflow.com/questions/22172861/>`__

  - If you don't want the remote copy feature, use ``--no-check-md5``.

- If you want some insurance against mistakes, such as mixing up the source and destination or forgetting trailing slash rules, you can add the option ``--no-delete-removed``, preventing the sync from deleting files.

- If you want some indication of progress besides when files are actually transferred, use ``--verbose``. This is recommended since progress output is done intelligently to reduce clutter, e.g. ``INFO: [1000/2368]   INFO: [2000/2368] ...``.

For the alpha to beta migration, you'll want to mind the mappings between the old and new directories. Here's an example set of commands:

::

  sudo s3cmd sync /cnhome/media/data/original/ s3://coralnet-production/media/images/ --verbose --no-delete-removed --no-check-md5
  sudo s3cmd sync /cnhome/media/label_thumbnails/ s3://coralnet-production/media/labels/ --verbose --no-delete-removed
  sudo s3cmd sync /cnhome/media/mugshots/ s3://coralnet-production/media/avatars/ --verbose --no-delete-removed

On 2016.11, re-syncing images took about 11 hours: 1 hour to get the remote file listing, and 10 hours to transfer the files. There were 772386 files total and 110223 files to transfer.


Failed attempt: Syncing files from a remote filesystem to S3
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

During the alpha to beta migration, we had to sync files from the non-AWS alpha server to S3. One idea was to run s3cmd from an AWS EC2 instance which was already associated with an IAM Role, thus saving the step of explicitly giving s3cmd the AWS keys. However, it didn't work out due to a hanging issue. The attempted procedure is described below:

SSH into an EC2 instance. Mount the CoralNet alpha server's filesystem using SSHFS.

- ``sudo apt-get install sshfs``
- ``sudo mkdir /mnt/cnalpha``
- ``sudo sshfs <username>@<alpha server host>:/ /mnt/cnalpha`` to mount the root of the alpha server's filesystem at ``/mnt/cnalpha``. Ensure that the alpha server's firewall accepts SSH (port 22) from this EC2 instance.

The sync commands become:

::

  sudo s3cmd sync /mnt/cnalpha/mnt/CoralNet/media/data/original/ s3://coralnet-production/media/images/ --verbose --no-delete-removed --no-check-md5
  sudo s3cmd sync /mnt/cnalpha/mnt/CoralNet/media/label_thumbnails/ s3://coralnet-production/media/labels/ --verbose --no-delete-removed
  sudo s3cmd sync /mnt/cnalpha/mnt/CoralNet/media/mugshots/ s3://coralnet-production/media/avatars/ --verbose --no-delete-removed

For ``images``, there is a chance that the sync will hang at the first step, compiling a list of local files. (Use ``--verbose`` to see whether it's on this step or not.) When doing the sync in 2016.07, this chance was maybe around 50%, but unfortunately in 2016.11 it seems to be 100%, making this syncing method no longer possible. The cause is unknown.

- Even ``sudo du -sh /mnt/cnalpha/mnt/CoralNet/media/data/original`` (this calculates the total filesize of the directory) cannot ever seem to complete, despite finishing in a few seconds when run directly on the alpha server.
- Also tried keeping the ``sshfs`` connection alive with ``-o reconnect,ServerAliveInterval=15,ServerAliveCountMax=3`` as suggested `here <http://stackoverflow.com/a/26584116/>`__, but it didn't help.


Resetting S3 file permissions
-----------------------------

Explanation
^^^^^^^^^^^

All S3 files should only be shown to a user if the website explicitly serves that file to a user (e.g. an image is displayed as part of a page). Otherwise, S3 files should be private to our AWS account.

The S3 bucket has a policy which enforces this by checking the Referer, as explained above. S3 buckets also have ACLs; these are the checkboxes you see when you click Properties -> Permissions for a bucket in the S3 console. The ACL should only grant permission to the name of our AWS account.

But that's not all - individual files in a bucket can specify ACLs too. For example, the default behavior of the ``django-storages`` third-party app is to save files with public-read ACLs. In the S3 console, this appears as a grant of the "Open/Download" permission to "Everyone". The ``AWS_DEFAULT_ACL`` setting must be set to ``'private'`` to prevent this grant from happening.

To be clear:

- Bucket policy says website referral required + File has private ACL = File requires website referral.

- Bucket policy says website referral required + File ACL allows public download = File can be publicly downloaded without website referral.

How to reset permissions
^^^^^^^^^^^^^^^^^^^^^^^^

If you notice or suspect that some bucket files have public-granting ACLs, this functionality from :ref:`s3cmd <s3cmd>` will reset all media files to private ACLs: ``s3cmd setacl --acl-private --recursive s3://<bucket-name>/media/``

As of 2016.11, this seems to take roughly 4-6 hours to complete.


Deleting an S3 bucket
---------------------

If you attempt to delete a large bucket from the S3 console, you'll get an error: ``There are more than 100000 objects (including versions) in <bucket name>.`` You need to use one of the methods `here <https://docs.aws.amazon.com/AmazonS3/latest/dev/delete-or-empty-bucket.html>`__ to delete the buckets. For a bucket that doesn't have versioning enabled, ``aws s3 rb s3://<bucket-name> --force`` should work.
