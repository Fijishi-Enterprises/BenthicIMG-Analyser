{% extends "base.html" %}
{% load static from staticfiles %}

{% block page-specific-includes %}
    {% include "static-local-include.html" with type="css" path="css/infopage.css" %}
{% endblock %}

{% block content %}


<div class="article-body center">
<a class="section-anchor section-anchor-top" data-scroll-name="#main"></a>
<h1>About CoralNet</h1>
    
    <p class="abstract">
        CoralNet is a repository and a resource for benthic images analysis.
        The site deploys deep neural networks which allow fully and semi automated annotation of images.
        It also serves as a repository and collaboration platform.
        CoralNet is free to use thanks to generous support from NSF and NOAA.
    </p>

    <ul class="indexlist">
    <li class="first-item"><a href="#story">story</a></li>
    <li><a href="#site">site</a></li>
    <li><a href="#datapolicy">data policy</a></li>
    <li><a href="#acknowledgments">acknowledgments</a></li>
    <li><a href="#people">people</a></li>
    <li><a href="#press">press</a></li>
    </ul>

    <h2 id="background">Story<a class="section-anchor" data-scroll-name="#story"></a></h2>
    <h4>Introduction</h4>
    <p>
        A catastrophic decline of biodiversity and coral cover is occuring at reefs across the world.
        To monitor the changes, and inform action plans, large spatiotemporal surveys are needed. Data
        collection methods are typically sufficient to meet this need but the subsequent image analysis
        requires manual inspection of each photo, creating manual annotation bottleneck.
    </p>
    <p>
        CoralNet reduces this bottleneck by deploying state of the art computer vision methods
        alongside human experts. Often 50-100% automation can be achieved with minimal reduction in the
        quality of the final data-product (see <a href="#acknowledgments">acknowledgments</a> for references).
        CoralNet, by its nature, also provides a platform for collaboration & sharing of data.
    </p>
    <h4>CoralNet Begins</h4>
    <p>
        The idea of CoralNet was conceived by Oscar Beijbom, then PhD student at the UCSD computer science department.
        Oscar was developing computer vision methods for coral reef images analysis and wanted to make sure the methods
        were made available to reef researchers, managers and enthusiasts.
        However, deploying such methods is not trivial. They require significant compute resources,
        integration to the user-interface has to be done just right, and they require a lot of training data.
        In particular, getting a hold of large amounts of well-organized annotation data was difficult.
        For all those reasons releasing code or developing a desktop application was not going to cut it,
        it needed to be a fully managed web-server.
    </p>
    <p>
        So CoralNet was born for the dual purpose of organizing the world's coral reef survey data, and
        using that data to create and deploy automated annotation methods.
    </p>
    <h4>CoralNet Alpha</h4>
    <p>
        The CoralNet Alpha server was developed by then UCSD undergrads Stephen Chan and Devang Sampat along with
        Oscar Beijbom. It first came online fall 2011 and funding came from NSF through a grant to PIs Greg Mitchell, David Kriegman and Serge Belongie.
        The Alpha server was single desktop computer housed at the UCSD computer science department.
        It was a scrappy effort but it worked!
    </p>
    <p>
        The computer vision backend of CoralNet Alpha deployed an
        <a href="http://vision.ucsd.edu/sites/default/files/automated_coral_annotation.pdf">
        early research method</a> which worked well, but not on par with human annotators.
        At the time, most users did not use the automated annotation engine, but were happy to have access to a free,
        easy-to-use, web-based annotation tool.
    </p>
    <h4>CoralNet Beta</h4>
    <p>
        A few years down the road there were over 300.000 images on CoralNet! The state of computer vision technology
        had also leapfrogged with the discovery of <a href="https://www.nature.com/articles/nature14539">deep learning</a>.
        We has also out-grown our desktop at UCSD.
        So the stars were aligned, and with funding from NOAA, we decided to release CoralNet Beta in November 2016.
    </p>
    <p>
        The beta launch was a significant efforts with three main changes.
        1) The server and database were moved to the cloud to enable continuous growth;
        2) the site UI and features received a large overhaul; and
        3) a much more powerful computer vision backend was deployed.
        The new backend is also hosted in the cloud and can engage 100 workers in parallel.
        Further the core algorithm was replaced by a much more powerful deep-learning based method.
        The method was presented at
        <a href="https://drive.google.com/file/d/1fr73WMMEOmca8QHy4Ejz6dcg4PNW_i6l/view?usp=sharing">ICRS 2016</a> and
        performs on par with human expert annotators. Interestingly, super-human accuracy was observed when the human
        experts collaborated with the autonomous system.
    </p>
    <p>
        CoralNet lead developer Stephen Chan led the software updates for the beta release, while Oscar Beijbom
        developed the computer vision backend.
        For full release notes refer to <a href="{% url 'release' %}">CoralNet Beta release-notes</a>.
    </p>
    <h4>CoralNet Beta 2 (Work in Progress!)</h4>
    <p>
        In early 2019
        <a href="https://www.frontiersin.org/articles/10.3389/fmars.2019.00222/full">Williams et al.</a> at NOAA
        conducted a large study and showed that the automated annotations for CoralNet Beta produced benthic cover
        estimates highly correlated with those derived from human annotators.
        This study thus suggests that CoralNet was finally accurate enough to address the manual annotation bottleneck!
    </p>
    <p>
        Following these results a second grant was received from NOAA to develop CoralNet Beta 2.
        This work will proceed along two main avenues.
        First, an API will be developed to allow the deployment of a trained classifier on vast number of images.
        This, in turn, enables the creation of a repository of trained classifiers from which users can choose from.
        Second, we will take another look at the core technology behind the computer vision system to see if we can push
        performance even further.
    </p>
    <p>
      CoralNet is open source. You can visit our <a href="https://github.com/beijbom/coralnet">GitHub repository</a> to see what we're working on, learn how the website works, or even contribute improvements.
    </p>

    <p class="uparrow border"><a href="#main">&#9650</a></p>

    <h2 id="site">The site<a class="section-anchor" data-scroll-name="#site"></a></h2>
    <p>
        The CoralNet website consist of several modules outlined below.
        For some (slightly outdated) information, check out
        our <a href="http://vimeo.com/channels/coralnet">Vimeo channel</a>.
    </p>
    <p>
        <b>Source:</b> Main organizational element for a benthic survey or image "source".
        Here you specify your labelset, your privacy settings, and invite collaborators.
    </p>
    <p>
        <b>Labelset:</b> Specify what labels you want to use in your analysis.
        Choose from a set of existing labels or create your own.
    </p>
    <p>
        <b>Import:</b> Upload images, metadata and archived annotations to the site.
    </p>
    <p>
        <b>Annotation:</b> Annotate your image right in the web browser using a point count interface.
        When enough images are manually annotated, an automated annotator is trained.
        This automated annotator is integrated directly into the annotation tool and
        makes the remaining annotation work easier.
    </p>

    <p class="uparrow border"><a href="#main">&#9650</a></p>

    <h2 id="datapolicy">Data Policy<a class="section-anchor" data-scroll-name="#datapolicy"></a></h2>

    <p>
        We at CoralNet highly encourage public sharing of research data for the greater good.
        However, we realize that data privacy is preferred in some cases, especially for newer projects.
        So, sources have two visibility options, Public or Private.
    </p>

    <h4>Public sources</h4>
    <ul>
      <li>All of your source's images and annotation data are available for the public
          to browse and download (including original images in full resolution).</li>
      <li>Only members of the source can add or edit content.</li>
    </ul>

    <h4>Private sources</h4>
    <ul>
      <li>All users can still see your source on the world map,
          based on the source's latitude and longitude settings.
          Here, they can also see the name, description, affiliation,
          and total number of images in your source - but no example images.</li>
      <li>Label information pages - public to all users - list the names of all sources that use the label,
          including private sources. Also, these label pages show example image 'patches'
          (small thumbnail cut-outs of a larger image) of point annotations using that label.
          These patches can come from any source using the label, but if a particular patch is from a private source,
          the name of the patch's source will not be given.</li>
      <li>Only members of the source (invited with View, Edit, or Admin permissions) can browse all of the
          source's data, including full images and their annotations.
          Other users will be unable to browse pages and images within the source,
          even if they know the URLs.</li>
    </ul>

    <h4>User privacy</h4>
    <ul>
      <li>Your username (handle) is public on CoralNet through the list of profiles.</li>
      <li>You may choose to make your user profile (including first name, last name, and affiliation)
          public for everyone, public for registered users only, or private.</li>
      <li>The email address associated with your account is not publicly viewable.</li>
      <li>We'll never ask you for your password. If you get an email asking you for your password, it didn't come from us.</li>
      <li>The CoralNet website uses cookies for login and for analytics (via Google).</li>
    </ul>

    <p class="uparrow border"><a href="#main">&#9650</a></p>
    
    <h2 id="acknowledgments">Acknowledgments<a class="section-anchor" data-scroll-name="#acknowledgments"></a></h2>
    <p>
        CoralNet has been supported from the following grants.
    </p>
    <ul>
    <li> 2012 - 2015: NSF <i>Computer Vision Coral Ecology grant #ATM-0941760</i> </li>
    <li> 2014 - 2016: NOAA <i>Grant #NA10OAR4320156 </i></li>
    <li> 2019 - 2020: NOAA <i>CoralNet: Tackling Bottlenecks in Coral Reef Image Analysis with
        Next Gen Deep Networks for Photographs to Large Mosaics.</i> </li>
    </ul>
    <p>
        The following papers are relevant to the development of CoralNet.
        We ask that you cite the appropriate papers if you use CoralNet in your work.
    </p>
    <ul>
    <li><p>
        O. Beijbom, P.J. Edmunds, D.I. Kline, G.B. Mitchell, D. Kriegman.
        <i><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6247798&tag=1">
            "Automated Annotation of Coral Reef Survey Images"</a></i>.
        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Providence, Rhode Island, June, 2012.</p></li>
    <li><p>
        O. Beijbom, P. J. Edmunds, C. Roelfsema, J. Smith, D. I. Kline, B. Neal, M. J. Dunlap,
        V. Moriarty, T-Y. Fan, C-J. Tan, S. Chan, T. Treibitz, A. Gamst, B. G. Mitchell, D. Kriegman.
        <i><a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130312">
            "Towards automated annotation of benthic survey images: variability of human experts and operational modes of automation"</a></i>.
        PLOS One, July 2015.
    </p></li>
    <li><p>
        I. D. Williams, C. S. Couch, O. Beijbom, T. A. Oliver, B. Vargas-Angel, B. D. Schumacher, R. E. Brainard.
        <i><a href="https://www.frontiersin.org/articles/10.3389/fmars.2019.00222/full">
            "Leveraging Automated Image Analysis Tools to Transform Our Capacity to Assess Status and Trends of Coral Reefs"</a></i>.
        Frontiers in Marine Science, May 2019.
    </p></li>
    <li><p>
        O. Beijbom,
        <i><a href="http://roger.ucsd.edu/record=b8984742~S9">
            "Automated Annotation of Coral Reef Survey Images"</a></i>.
        PhD Thesis UCSD, June 2015.
    </p></li>
    </ul>

    <p class="uparrow border"><a href="#main">&#9650</a></p>
    <h2 id="people">People<a class="section-anchor" data-scroll-name="#people"></a></h2>
    <h3>Current</h3>
        <ul>
            <li>Oscar Beijbom - Director [<a href="https://beijbom.github.io/">www</a>]</li>
            <li>Stephen Chan - Lead Developer </li>
            <li>David Kriegman - Academic advisor [<a href="http://cseweb.ucsd.edu/~kriegman/">www</a>]</li>
            <li>Qimin Chen - Vision researcher </li>
            <li>Jessica Bouwmeester - LabelSet curation [<a href="http://jessbm.weebly.com">www</a>]</li>
        </ul>
    <h3>Alumni</h3>
        <ul>
            <li>Serge Belongie - Academic advisor [<a href="https://vision.cornell.edu/se3/people/serge-belongie/">www</a>]</li>
            <li>David Kline - Academic advisor [<a href="http://scrippsscholars.ucsd.edu/dkline/biocv">www</a>]</li>
            <li>Ben Neal - Academic advisor [<a href="https://www.livingoceansfoundation.org/profile/bneal/">www</a>]</li>
            <li>Gregory Mitchell - Academic advisor [<a href="http://spg.ucsd.edu/People/Greg/">www</a>]</li>
            <li>Devang Sampat - Developer</li>
            <li>Andrew Hu - Developer</li>
            <li>Jeff Sandvik - Developer</li>
        </ul>

    <p class="uparrow border"><a href="#main">&#9650</a></p>
    <h2 id="press">Press<a class="section-anchor" data-scroll-name="#press"></a></h2>
    <ul >
    <li>January 2017: Our beta launch featured in phys.org
        [<a href="http://phys.org/news/2017-01-software-coral-reef-images.html">www</a>]. </li>

    <li>September 2016: CoralNet featured in the GTC keynote (at 0:0:52)
        [<a href = "https://www.youtube.com/watch?v=npzRyTimcZo">www</a>]. </li>

    <li>August 2016: CoralNet featured in the Nature Toolbox section
        [<a href = "http://www.nature.com/news/computers-on-the-reef-1.20497">www</a>]. </li>

    <li>June 2016: NVIDIA blog post about Oscar's work on deep learning for coral ecology
        [<a href = "https://blogs.nvidia.com/blog/2016/06/22/deep-learning-save-coral-reefs/">www</a>]. </li>

    <li >Nov 2014: Jonathan Cohen at NVIDIA highlights CoralNet in his talk at the SuperComputer conference.
        CoralNet section starts at 9.30
        [<a href="http://on-demand.gputechconf.com/supercomputing/2014/video/SC411-machine-learning-computational-researchers.html">www</a>].</li>

    <li >May 2014: Destin at SmarterEveryDay follows the data collected by the Catlin Seaview Survey
        all the way to CoralNet [<a href="https://www.youtube.com/watch?v=az1PTIehYKI">www</a>].</li>
            
    <li >September 2013: Greenwire covers CoralNet
        [<a href="http://www.eenews.net/greenwire/stories/1059986651">www</a>]. </li>
    </ul>

    <p class="uparrow-final border"><a href="#main">&#9650</a></p>
</div>

<script>
$("a[href^='#']").click(function(){
    var clicked = $(this).attr("href");
    var link = $('a[data-scroll-name="' + clicked + '"]');
    
    $(".selected").attr("class","");
    
    $('body,html').animate({
        'scrollTop':   $(link).offset().top
    }, 500, function() {
    // Animation complete.
    });
    $(this).attr("class"," selected");
});
</script>
{% endblock %}

