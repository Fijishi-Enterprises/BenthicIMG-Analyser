Production server setup
=======================
This page contains setup procedures for the AWS production server.


IAM user
--------
Log into your AWS account and go to the Amazon IAM dashboard.

Create an IAM user. See `this Amazon docs page <http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html#create-an-iam-user>`__ for details.

Now you can log into AWS using your IAM user credentials, rather than your Amazon account credentials.


RDS instance
------------
Go to the Amazon EC2 dashboard. Create a Security Group which allows inbound connections on port 5432 (PostgreSQL) from the machine you are working from. (Use a site like `whatismyip.com <https://www.whatismyip.com/>`__ to find your IP.)

Go to the Amazon RDS dashboard. Create a PostgreSQL 9.5.x RDS instance.

- Assign the Security Group that you just created.
- Select Yes on the Publicly Accessible option.
- Make sure the Database Port is the default, 5432.

Once the RDS instance is created, in your local pgAdmin, log in as the master user you created.

Now using pgAdmin, follow the Installation page's :ref:`installation-postgresql` section to set up the ``coralnet`` database and ``django`` user.

- The ``coralnet`` database is already created, but needs the correct settings. Also, set ``django`` as the owner of the ``coralnet`` database, and as the owner of the ``public`` schema within ``coralnet``.

If doing the 2016 migration process:

- Edit the Security Group to also allow port-5432 connections from the machine running pgloader.
- Follow the instructions at this section: :ref:`y2016-migration-pgloader`


S3 bucket
---------

- Go to the Amazon S3 console.
- Create a bucket.
- Click your bucket's name, then click Properties, then Permissions.
- Click "Add CORS Configuration" and accept the default configuration.
- Click "Add bucket policy" and add the following policy, replacing ``<bucket-name>`` with your bucket's name and ``<mydomain.com>`` with your website's domain name:

::

  {
    "Version": "2012-10-17",
    "Statement": [
      {
        "Sid": "Allow referral from my domains",
        "Effect": "Allow",
        "Principal": "*",
        "Action": "s3:GetObject",
        "Resource": "arn:aws:s3:::<bucket-name>/*",
        "Condition": {
          "StringLike": {
            "aws:Referer": "http://<mydomain.com>/*"
          }
        }
      }
    ]
  }

Explanation
...........

The default CORS configuration should allow any website domain to include bucket objects in their webpages, provided that they have read access to those bucket objects in the first place. CORS is required because our actual website isn't on S3; therefore including an S3 resource on our website is cross-origin sharing.

The above bucket policy defines how the bucket objects can be accessed. It says that, if a bucket object is linked to from the given referer website, that object can be read. In other cases, access is restricted to IAM users as defined in the IAM console (as long as the website code only writes S3 ACLs with private permissions).

This bucket policy prevents hotlinking to S3 images, meaning that if a user just types an image URL into their address bar, they will be denied access. For example, if a user sees an example patch from a private source with filename ``abcdefghij.jpg.150x150.jpg``, and then they try to reach the full-resolution image by simply cutting off the ``.150x150.jpg`` from the URL, they will be denied access.

Here's how it works: an S3 image is served with extra URL arguments after the filename, namely ``Signature=<letters and numbers>``, ``Expires=<number>``, and ``AWSAccessKeyId=<letters and numbers>``. The Signature in particular is generated by S3 during the website request, and is the key that allows the user to read the image. The Signature only works for serving that particular file until that particular Expires time (``django-storages`` makes it expire in 1 hour by default).

Note that although the AWS access key ID is exposed in the URL, it's not particularly a security issue, since an attacker cannot do anything unless they have the secret key as well. (`Link <http://stackoverflow.com/questions/7678835/how-secure-are-amazon-aws-access-keys>`__)

One potential security hole is the fact that ``HTTP_REFERER`` can be set by the web client. Most web clients provide valid referers, so it would take a reasonably focused snooper to take advantage of the security hole. Still, there are ways of restricting access further that could be implemented, such as using CloudFront and CNAMEs. See the comments in `this thread <http://stackoverflow.com/a/11525941/>`__.

See `this blog post <https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/>`__ for info on bucket policies, `this docs page <http://docs.aws.amazon.com/AmazonS3/latest/dev/manage-acls-using-console.html>`__ for info on ACLs, and `this docs page <http://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-4>`__ for the referer check.


EC2 Linux instance
------------------
Go to the Amazon EC2 dashboard. Create a Security Group which allows:

- Inbound SSH connections (port 22) from the machine you are working from. (Use a site like `whatismyip.com <https://www.whatismyip.com/>`__ to find your IP.)
- Inbound HTTP and HTTPS connections from all IPs.

Use the dashboard to create a new EC2 instance.

- On the "Configure Instance Details" step, check "Protect against accidental termination".
- On the "Add Storage" step, uncheck "Delete on Termination".

Edit the RDS instance's Security Group to allow inbound connections from the EC2 instance's Security Group. (This allows Django to connect to the database.) In the Source box, type ``sg`` and the security group choices should appear.

Create a key pair for your IAM user if you haven't already, and SSH into the EC2 instance using the key pair.

- See `this Amazon docs page <http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html#create-a-key-pair>`__ for details on creating and configuring the key pair.
- Check the instance on the EC2 dashboard, and find its Public DNS. Use that as the host name to SSH into.
- Log in with the EC2 instance's default username. The username varies depending on which Linux you're using: `Link <http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectingPuTTY>`__

Once you're in the SSH session, upgrade system packages: ``sudo apt-get update`` then ``sudo apt-get upgrade`` on Ubuntu. Log out. Go to the EC2 dashboard and reboot the EC2 instance. Log in again.

Create a user for each person, using ``sudo adduser <username> --disabled-password`` and then following the rest of the instructions in `this Amazon guide <http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/managing-users.html>`__.

- The guide explains that it's insecure to share the default user (``ubuntu`` in this case) between multiple people, because "that account can cause a lot of damage to a system when used improperly."

For each of those users, allow them to use ``sudo`` without specifying a password (since requiring a private key login makes a password redundant):

- Add the user to the sudoers group: ``sudo usermod -a -G sudo <username>``
- Run ``sudo visudo -f /etc/sudoers.d/mysudoers`` to edit a new sudoers file called ``mysudoers``. Add the line: ``<username> ALL=NOPASSWD: ALL`` (`Source <http://superuser.com/a/869145/>`__)

  - If you are adding multiple users, just use the same sudoers file for both users.
  - FYI, the default ``ubuntu`` user doesn't require a sudo password by default. That's configured in ``/etc/sudoers.d/90-cloud-init-users``. (`Source <http://askubuntu.com/questions/309418/make-an-amazon-ec2-instance-ask-for-sudoing-password>`__) This is because the ``ubuntu`` user doesn't have a login password by default, since as previously mentioned, a private key requirement makes a password redundant.

Create a ``/srv/www`` directory for putting website files. (This seems to be a recommended, standard location: `Link 1 <http://serverfault.com/questions/102569/should-websites-live-in-var-or-usr-according-to-recommended-usage>`__, `Link 2 <http://superuser.com/questions/635289/what-is-the-recommended-directory-to-store-website-content>`__)

- Change the directory's group to ``www-data``: ``sudo chgrp www-data www``
- Add your user to the ``www-data`` group: ``sudo usermod -aG www-data usernamegoeshere``
- Check that you did it right: ``cat /etc/group``
- If you are currently signed in as that user, logout and login to make the new permissions take effect. (`Source <http://unix.stackexchange.com/questions/96343/how-to-take-effect-usermod-command-without-logout-and-login>`__)
- Allow group write permissions: ``sudo chmod g+w www``
- Make all new files created in the ``www`` directory have their group set to the directory's group: ``sudo chmod g+s www``

Follow the :doc:`installation` page, putting the project files in ``/srv/www``. Make these adjustments to the instructions:

- Skip the PostgreSQL section; that was for the RDS instance, not this instance
- The Django ``DATABASES`` setting should match the RDS instance configuration
- If doing the 2016 migration process, go here for Django migration instructions: :ref:`y2016-migration-django-migrations`
- Skip the sections marked "(dev only)"
- When running ``runserver``, use an `SSH tunnel <http://www.sotechdesign.com.au/browsing-the-web-through-a-ssh-tunnel-with-firefox-and-putty-windows/>`__ to view the website. Make sure your browser's proxy settings do NOT exclude localhost or 127.0.0.1 from the SSH tunnel.


Web server
----------
Our current web server setup involves running gunicorn and nginx on the EC2 instance.

gunicorn
........
Activate your virtualenv. If you used the production requirements file, you should already have gunicorn installed. If not, run ``pip install gunicorn``.

``cd /srv/www/coralnet/project``. Change your Django settings to ``DEBUG = True`` for a start. Run ``gunicorn config.wsgi:application``. Check 127.0.0.1:8000 from an SSH tunnel to see if it worked.

Now change your Django settings to ``DEBUG = False``, and then run the same command: ``gunicorn config.wsgi:application``. Check 127.0.0.1:8000 from an SSH tunnel to see if loading pages works. If you want to make things easier for now, change two Django settings: ``ADMINS = []`` and ``ALLOWED_HOSTS = [<other entries>, '127.0.0.1']``.


nginx
.....
``sudo apt-get install nginx``.

Run ``sudo /etc/init.d/nginx start``. On your local machine, try entering the EC2 instance's public DNS in your browser's address bar. You should see a "Welcome to nginx!" page.

Allow nginx to find our configuration file, enable it, and disabled the default site's configuration file (`Source <http://serverfault.com/a/424456>`__):

::

  sudo ln -s /srv/www/coralnet/project/config/nginx.conf /etc/nginx/sites-available/coralnet
  sudo ln -s /etc/nginx/sites-available/coralnet /etc/nginx/sites-enabled
  sudo rm /etc/nginx/sites-enabled/default
  
- Try browsing `the nginx docs <http://nginx.org/en/docs/beginners_guide.html>`__ if you're wondering how nginx config works.


Running
.......
Restart nginx: ``sudo /etc/init.d/nginx restart``. (Other commands are ``start``, ``stop``, and ``status``.)

Run gunicorn, this time binding it to localhost on port 8001, and setting a longer timeout than the default 30s: ``gunicorn config.wsgi:application --bind=127.0.0.1:8001 --timeout 604810 &``

Again, on your local machine, enter the EC2 instance's public DNS in your browser's address bar. You should see the CoralNet website.

From here on out:

- Remember that you need both nginx and gunicorn up and running for the website to work.
- To update the Django code, kill the gunicorn master process, then update the code, then start gunicorn again.
- Remember that gunicorn must be run in the virtualenv, and also run from the correct directory (``coralnet/project``) so that ``config.wsgi`` can be found.
- If the server's timeout duration needs to be adjusted, you should adjust both the gunicorn ``--timeout`` option as well as nginx's ``proxy_read_timeout`` option in ``coralnet/project/nginx.conf``.

  - If ``gunicorn`` times out, the browser gets a "502 Bad Gateway" page. If ``nginx`` times out, the browser gets a "504 Gateway Time-out" page.
  - The gunicorn and nginx timeouts should be similar but slightly different, so that the timeout error is consistent (either always nginx or always gunicorn, not one or the other).
  - Note that ``nginx.conf`` is under our project's version control.


Previous attempts at web server setup
-------------------------------------


Apache
......
The following is based on `Apache's installation guide <https://httpd.apache.org/docs/2.4/install.html>`__.

Download PCRE from `here <http://www.pcre.org/>`__. Extract it.

- These instructions include PCRE 1, not 2. Using 2 seems to get stuck at the httpd ``make`` step, as it tries to find ``pcre.h`` while the file you have is ``pcre2.h``.

``cd`` into the extracted PCRE directory, and run:

::
    
  ./configure
  make
  sudo make install

Download Apache httpd from their `website <http://httpd.apache.org/download.cgi>`__. Extract it.

Download Apache Portable Runtime (APR) from `here <http://apr.apache.org/>`__. Extract it into ``srclib/apr`` under the ``httpd`` source tree that you just extracted. For example: ``tar xzvf apr-1.5.2.tar.gz -C httpd-2.4.20/srclib`` then ``mv httpd-2.4.20/srclib/apr-1.5.2 httpd-2.4.20/srclib/apr``.

Download APR-Util from the same page. Extract it into ``srclib/apr-util`` under the ``httpd`` source tree. For example: ``tar xzvf apr-util-1.5.4.tar.gz -C httpd-2.4.20/srclib`` then ``mv httpd-2.4.20/srclib/apr-util-1.5.4 httpd-2.4.20/srclib/apr-util``.
  
Now ``cd`` into the ``httpd`` directory, and run:

::
  
  ./configure --with-included-apr
  make
  sudo make install
  
Also get:

- The dev package for Apache: ``sudo apt-get install apache2-dev`` on Ubuntu.
- The ``lynx`` text-based browser, which allows you to see Apache's status: ``sudo apt-get install lynx`` on Ubuntu.

You may want to add the directory containing ``apachectl`` to the ``PATH`` environment variable. To modify the ``PATH`` that a sudoer sees on Ubuntu, run ``sudo visudo`` and modify the ``secure_path`` line. (`Source <http://stackoverflow.com/a/4572018>`__)


mod_wsgi
........
Get mod_wsgi from the source code link `here <https://modwsgi.readthedocs.io/en/develop/user-guides/quick-installation-guide.html>`__. Extract it.

``cd`` into the extracted mod_wsgi directory, and run:

::
    
  ./configure
  make
  sudo make install

Locate the Apache config file, such as ``/usr/local/apache2/conf/httpd.conf``. Add this line to the file, at the same point that other Apache modules are being loaded: ``LoadModule wsgi_module /usr/lib/apache2/modules/mod_wsgi.so`` (Edit the last option according to where ``mod_wsgi.so`` is located.)


Django configuration of Apache + mod_wsgi
.........................................
Edit ``httpd.conf`` to include:

::
    
  # Django - Serve static files from local directories.
  # Format: Alias STATIC_URL STATIC_ROOT
  # <Directory STATIC ROOT>
  
  Alias /static/ /srv/www/static_serve/
  
  <Directory /srv/www/static_serve>
  Require all granted
  </Directory>
  
  # Django - Specify the WSGI script, and ensure that our apps and 3rd-party
  # Python apps can be imported.
    
  #WSGIDaemonProcess coralnet python-path=/srv/www/coralnet/project:/srv/www/virtenv_coralnet/lib/python2.7/site-packages
  #WSGIProcessGroup coralnet
  #WSGIScriptAlias / /srv/www/coralnet/project/config/wsgi.py process-group=coralnet
  
  WSGIScriptAlias / /srv/www/coralnet/project/config/wsgi.py
  WSGIPythonPath /srv/www/coralnet/project:/srv/www/virtenv_coralnet/lib/python2.7/site-packages
  
  <Directory /srv/www/coralnet/project/config>
  <Files wsgi.py>
  Require all granted
  </Files>
  </Directory>
  
  # Allow mod_wsgi to use daemon mode on this system.
  # http://modwsgi.readthedocs.io/en/develop/user-guides/configuration-issues.html#location-of-unix-sockets
  
  #WSGISocketPrefix run/wsgi
  
  
Why Apache + mod_wsgi was a dead end so far
...........................................
We kept getting this 500 error when loading any page: ``ImproperlyConfigured: Error loading psycopg2 module: /srv/www/virtenv_coralnet/lib/python2.7/site-packages/psycopg2/_psycopg.so: undefined symbol: PyUnicodeUCS2_AsUTF8String``

`An SO thread <http://stackoverflow.com/questions/36129828/improperlyconfigured-error-importing-middleware-django-wsgi-error-apache>`__ suggested specifying ``WSGIPythonHome`` in the Apache config to explicitly point to the virtualenv's Python.

However, when we did this, we got a different error:

::
  
    File "/ ... /python2.7/hmac.py", line 8, in <module>
      from operator import _compare_digest as compare_digest
  ImportError: cannot import name _compare_digest

`This SO thread <http://stackoverflow.com/questions/24853027/django-importerror-cannot-import-name-compare-digest>`__ suggested recreating the virtualenv. However, when we did that, we were stuck with the same error.

Some possible troubleshooting steps from here include:

- Try apache + mod_wsgi with coralnet and a virtualenv based on the system's default Python (which is outdated, 2.7.6).
- Try apache + mod_wsgi with a bare Django project.
- Try apache + mod_wsgi with a Django project that's bare other than using PostgreSQL.


Elastic Beanstalk
.................
These instructions are mainly from the `tutorial on deploying Django with Elastic Beanstalk <https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-django.html>`__.

In your EC2 instance, install the Elastic Beanstalk command-line interface: ``sudo pip install awsebcli``

``cd /srv/www/coralnet/project`` then ``eb init -p python2.7 coralnet``. It'll ask for credentials. Check the IAM Dashboard under Security Credentials for the access ID. It won't let you view the secret key again though; you'll need to have that saved.

- The directory you run ``eb init`` in will end up having an ``.elasticbeanstalk`` directory.

If you want to be able to SSH into the instance running your application, run ``eb init`` again and select your keypair at the prompt.

``eb create coralnet-env`` to create a load-balanced Elastic Beanstalk environment. This will take about 5 minutes to complete.

Check ``eb status``. The ``CNAME`` is a public URL for the website. Copy and paste it into your browser's URL bar to see the website.

- You can also find the EB environment's URL with the Elastic Beanstalk dashboard.

- To get a better handle on what has been deployed, you can go to the EB dashboard and look under Application Versions for your EB application. Click a Source archive to download it.

- To get a better handle on the deployed environment's status, click the environment in the EB dashboard. (Should be a green box, or a different color depending on the "health" of the environment.)

- To see logs, try ``eb logs`` or go to the EB dashboard to view the environment's Logs. ``error_log`` should have info for 500 errors.

- From now on, after you change any code, you'll be able to re-deploy the website using ``eb deploy``.


Why Elastic Beanstalk didn't work out so far
............................................
Deploying EB with its Python framework is somewhat inflexible. It demands that the Python requirements file must be installed in ``requirements.txt`` at the root of the environment container. Up to this point, we haven't found a place to tell EB to run commands (such as ``cp config/requirements/production.txt requirements.txt``) prior to the Python packages being installed. So, we would have to manually copy the requirements.txt file over to the required location for purposes of deployment, and perhaps put this path in the ``.gitignore``. We haven't bothered getting this to work yet.

Besides that, there are numerous Linux packages that must be installed to get some of our Python packages working, particularly Pillow and psycopg2. These installations must be specified in EB's configuration files. However, to check if the EB configuration works, we have to deploy an EB instance, which takes around 5 minutes to complete. If we have one attempt at configuration every 5 minutes, we really need to know exactly what we're doing to maintain our sanity. We're probably not at this point yet.

One possible alternate route is to use EB's Dockerfile framework instead of its Python framework. This could potentially be easier to test outside of EB, and should offer more flexibility compared to EB's Python framework. It also ties most of our setup details to the popular Docker software rather than to EB.
